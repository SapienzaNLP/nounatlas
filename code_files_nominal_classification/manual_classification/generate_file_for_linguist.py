import sys, os, csv, json
import openpyxl

import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')

from typing import Union
import random
import argparse

DIR = os.path.realpath(os.path.dirname(__file__))
ROOT_PATH = os.path.join(DIR, '../../')

# region parameters

pipeline_phase: Union['generate','parse', 'generate_manual_validation'] = 'parse'
# generate -> we create the two files needed for the linguist
# parse -> we obtain the metrics from the annotated file of the linguist and a csv
# generate_manual_validation -> we use a subset of synsets from the "generate" phase to do a double-check

va_frame_infos_path = os.path.join(ROOT_PATH, 'resources/VerbAtlas/VA_frame_info.tsv')
results_tsv_path = os.path.join(ROOT_PATH, 'outputs_nominal_classification/results_crossencoderclassifier_2023-12-19_16-33-31/results.tsv')
verbals_json_path = os.path.join(ROOT_PATH, 'datasets/dataset_nominal_classification/verbals.json')
linguistic_file_ouput_dir = os.path.join(ROOT_PATH, 'outputs_nominal_classification/results_linguist/')
linguistic_file_name = 'results_for_expert'
frames_infos_file_name = 'frames_infos'

# parameters for manual validation:
manual_validation_number_of_synsets = 100
validator_file_name = 'results_for_validation'
seed = "N"

# endregion

# region functions

def open_va_infos(va_frame_infos_path):
    va_frame_infos = {}
    with open(va_frame_infos_path, mode="r", newline="") as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter="\t")
        # Read the header row
        _ = next(tsv_reader)
        # Read the data rows
        for row in tsv_reader:
            va_frame_infos[row[1]] = {
                "use_case": row[2],
                "syn_def_example": row[4],
                "syn_examples": row[5],
                "def_formatted": f'{row[4]} (e.g. {row[5]}). {row[2]}'
            }
    return va_frame_infos

def open_results_tsv(results_tsv_path):
    results_tsv = {}
    with open(results_tsv_path, mode="r", newline="") as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter="\t")
        # Read the header row
        _ = next(tsv_reader)
        # Read the data rows
        for row in tsv_reader:
            synset_name, synset_definition = row[0], row[1]
            frames = row[2:]
            results_tsv[synset_name] = {
                "synset_name": synset_name,
                "definition": synset_definition,
                "frames": frames
            }
    return results_tsv

def open_verbal_dataset(verbal_json_path):
    with open(verbal_json_path, "r") as f:
        data = json.load(f)["verbal_definitions"]
    return data

def save_as_excel(data, save_path):
    workbook = openpyxl.Workbook() # Create a new Excel workbook
    sheet = workbook.active # Get the active sheet
    for row in data:
        sheet.append(row)
    workbook.save(save_path)

# endregion

# region terminal parameters

if __name__ == "__main__" and len(sys.argv) > 1:
    parser = argparse.ArgumentParser(description="This script manages different phases of the semi-automatic annotation done by the linguist (see Section 3.1.4 of the original paper).")
    parser.add_argument(
        "--pipeline_phase",
        type=str,
        default="generate",
        required=True,
        choices=['generate','parse', 'generate_manual_validation'],
        help="generate -> we create the two files needed for the linguist\nparse -> we obtain the metrics from the annotated file of the linguist and a csv\ngenerate_manual_validation -> we use a subset of synsets from the 'generate' phase to do a double-check"
    )

    parser.add_argument(
        "--model_results_path",
        type=str,
        default=results_tsv_path,
        required=False,
        help="the path for the results generated by the model (e.g. python code_files_nominal_classification/crossencoder_main.py --pipeline_phase predict --version_name version_0)."
    )

    args = parser.parse_args()
    pipeline_phase = args.pipeline_phase
    results_tsv_path = args.model_results_path

# endregion

if pipeline_phase == 'generate' or pipeline_phase == 'generate_manual_validation':
    # Open frame informations files
    va_frame_infos = open_va_infos(va_frame_infos_path)
    results_tsv = open_results_tsv(results_tsv_path)
    verbals_json = open_verbal_dataset(verbals_json_path)

    if pipeline_phase == 'generate_manual_validation':
        random.seed(seed)
        results_tsv_random = list(results_tsv.keys())
        results_tsv_random = random.sample(results_tsv_random, manual_validation_number_of_synsets)
        results_tsv = {k:v for k,v in results_tsv.items() if k in results_tsv_random}
        linguistic_file_name = validator_file_name + "_" + str(seed)

    # Process results file for expert
    ouput_data = []
    for synset_target, synset_infos in results_tsv.items():
        # Synset name and definition:
        ouput_data.append([
            synset_target, 
            ", ".join([s.name().replace("_", " ") for s in wordnet.synset(synset_target).lemmas()]), 
            wordnet.synset(synset_target).definition()
        ])
        # Predicted frames:
        for possible_frame in synset_infos["frames"]:
            ouput_data.append([
                '',
                possible_frame,
                va_frame_infos[possible_frame]["def_formatted"]
            ])

    save_as_excel(ouput_data, os.path.join(linguistic_file_ouput_dir, f'{linguistic_file_name}.xlsx'))

    # Process frames file info with verbal synsets for expert
    ouput_data = []
    for frame_name, v_synsets in sorted(verbals_json.items()):
        # Frame name and definition:
        ouput_data.append([
            frame_name, 
            '', 
            va_frame_infos[frame_name]['syn_def_example'], 
            va_frame_infos[frame_name]['use_case']
        ])
        # All its verbal syns:
        for v_synset_name, v_synset_info in sorted(v_synsets.items()):
            ouput_data.append([
                '', 
                v_synset_name, 
                ", ".join([s.name().replace("_", " ") for s in wordnet.synset(v_synset_name).lemmas()]), 
                wordnet.synset(v_synset_name).definition(),
            ])
    save_as_excel(ouput_data, os.path.join(linguistic_file_ouput_dir, f'{frames_infos_file_name}.xlsx'))

    print("Files generated:")
    print(os.path.join(linguistic_file_ouput_dir, f'{linguistic_file_name}.xlsx'))
    print(os.path.join(linguistic_file_ouput_dir, f'{frames_infos_file_name}.xlsx'))

elif pipeline_phase == 'parse':
    workbook = openpyxl.load_workbook(os.path.join(linguistic_file_ouput_dir, f'{linguistic_file_name}.xlsx')) # Load the Excel workbook
    sheet = workbook.active # Get the active sheet

    excel_data = []
    for row in sheet.iter_rows(values_only=True):
        excel_data.append(list(row))

    disambiguated_data = {}
    ambiguous_data = {}
    not_classified_data = []
    synset_target = None
    frame_targets = None
    synsets_in_ambiguity = 0
    for i in range(len(excel_data)):
        cells = ['' if c is None else c.strip() for c in excel_data[i]]

        if len(cells[0].split('.')) == 3: # row of a synset

            # if synset and frame(s) were found:
            if frame_targets != None:
                if len(frame_targets) == 1:
                    disambiguated_data[synset_target] = frame_targets[0]
                elif len(frame_targets) > 1:
                    ambiguous_data[synset_target] = frame_targets
                elif len(frame_targets) == 0:
                    print(f'[WARN] {synset_target} is not classified')
                    not_classified_data.append(synset_target)

            synset_target = cells[0] # new synset target
            frame_targets = []

        elif cells[0] != '': # if that frame is selected as possibility
            frame_targets.append( cells[1] )

        i+=1

    # Generate the file tsv with the resulting disambiguated data:
    with open(os.path.join(linguistic_file_ouput_dir, f'results_from_expert.tsv'), 'w') as tsv_file:
        for synset_name, frame_name in disambiguated_data.items():
            definition = wordnet.synset(synset_name).definition()
            row_values = f"{synset_name}\t{definition}\t{frame_name}"
            tsv_file.write(row_values + "\n")

    results_tsv = open_results_tsv(results_tsv_path)

    print("Total: ", len(disambiguated_data) + len(ambiguous_data) + len(not_classified_data))
    print("Results length file: ", len(results_tsv))
    print("Disambiguated len: ", len(disambiguated_data))
    print("Ambiguous len: ", len(ambiguous_data))
    print("Not classified len: ", len(not_classified_data))

    # Checking how much the top-k were used:
    
    print("Disambiguated using top-k: ", len([
        synset for synset, frame in disambiguated_data.items() 
        if frame in results_tsv[synset]["frames"]]
    ))
    print("Disambiguated using top-1: ", len([
        synset for synset, frame in disambiguated_data.items() 
        if frame == results_tsv[synset]["frames"][0]]
    ))

    print("Done")
